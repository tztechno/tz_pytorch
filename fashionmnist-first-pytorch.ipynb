{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This notebook referred to DATAI's great notebook 'Pytorch Tutorial for Deep Learning Lovers'\nhttps://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"../input\"))","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.037242,"end_time":"2021-10-15T17:36:28.453569","exception":false,"start_time":"2021-10-15T17:36:28.416327","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-16T04:18:52.005788Z","iopub.execute_input":"2021-10-16T04:18:52.006576Z","iopub.status.idle":"2021-10-16T04:18:52.011994Z","shell.execute_reply.started":"2021-10-16T04:18:52.006535Z","shell.execute_reply":"2021-10-16T04:18:52.011207Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","metadata":{"_cell_guid":"a0bf0fa7-c527-4fd3-b504-02a88fc94798","_uuid":"1382c63fe24710d3b2840e7dcf172cddbf533743","papermill":{"duration":2.373975,"end_time":"2021-10-15T17:36:30.845575","exception":false,"start_time":"2021-10-15T17:36:28.471600","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-16T04:18:52.013704Z","iopub.execute_input":"2021-10-16T04:18:52.014209Z","iopub.status.idle":"2021-10-16T04:18:52.024972Z","shell.execute_reply.started":"2021-10-16T04:18:52.014163Z","shell.execute_reply":"2021-10-16T04:18:52.023608Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Dataset","metadata":{}},{"cell_type":"code","source":"# Prepare TRAIN Dataset\n# load data\ntrain = pd.read_csv(r\"../input/fashionmnist/fashion-mnist_train.csv\",dtype = np.float32)\n\n# split data into features(pixels) and labels(numbers from 0 to 9)\ntargets_numpy = train.label.values\nfeatures_numpy = train.loc[:,train.columns != \"label\"].values/255 # normalization\n\n# train test split. Size of train data is 80% and size of test data is 20%. \nfeatures_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n                                                                             targets_numpy,\n                                                                             test_size = 0.2,\n                                                                             random_state = 42) \n\n# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\nfeaturesTrain = torch.from_numpy(features_train)\ntargetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n\n# create feature and targets tensor for test set.\nfeaturesTest = torch.from_numpy(features_test)\ntargetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n\n# batch_size, epoch and iteration\nbatch_size = 100\nn_iters = 10000\nnum_epochs = n_iters / (len(features_train) / batch_size)\nnum_epochs = int(num_epochs)\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n\n# visualize one of the images in data set\nplt.imshow(features_numpy[10].reshape(28,28))\nplt.axis(\"off\")\nplt.title(str(targets_numpy[10]))\nplt.savefig('graph.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-16T04:18:52.027002Z","iopub.execute_input":"2021-10-16T04:18:52.027601Z","iopub.status.idle":"2021-10-16T04:18:58.283653Z","shell.execute_reply.started":"2021-10-16T04:18:52.027566Z","shell.execute_reply":"2021-10-16T04:18:58.282548Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"# Prepare TEST Dataset\n# load data\nTEST0 = pd.read_csv(r\"../input/fashionmnist/fashion-mnist_test.csv\",dtype = np.float32) \n\n# split data into features(pixels) and labels(numbers from 0 to 9)\nTEST_targets_numpy = TEST0.label.values\nTEST_features_numpy = TEST0.loc[:,TEST0.columns != \"label\"].values/255 # normalization\n\nfeatures_TEST = TEST_features_numpy\ntargets_TEST = TEST_targets_numpy\n\n# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\nfeaturesTEST = torch.from_numpy(features_TEST)\ntargetsTEST = torch.from_numpy(targets_TEST).type(torch.LongTensor) \n\n# Pytorch train and test sets\nTEST = torch.utils.data.TensorDataset(featuresTEST,targetsTEST)\n\n# data loader\nTEST_loader = DataLoader(TEST, batch_size = batch_size, shuffle = False)\n\n# visualize one of the images in data set\nplt.imshow(TEST_features_numpy[10].reshape(28,28))\nplt.axis(\"off\")\nplt.title(str(TEST_targets_numpy[10]))\nplt.savefig('graph.png')\nplt.show()","metadata":{"_cell_guid":"59cdc9d5-da8f-4d7a-abc5-c62b0008afb0","_uuid":"c6e0d7d3843719091564a580dbe08f67ee0d93ec","papermill":{"duration":4.835326,"end_time":"2021-10-15T17:36:35.699953","exception":false,"start_time":"2021-10-15T17:36:30.864627","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-16T04:18:58.285291Z","iopub.execute_input":"2021-10-16T04:18:58.286259Z","iopub.status.idle":"2021-10-16T04:18:59.199510Z","shell.execute_reply.started":"2021-10-16T04:18:58.286190Z","shell.execute_reply":"2021-10-16T04:18:59.198375Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"print(type(targetsTEST))\nprint(type(featuresTEST))\nprint(TEST)\nprint(TEST_loader)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T04:18:59.201590Z","iopub.execute_input":"2021-10-16T04:18:59.201828Z","iopub.status.idle":"2021-10-16T04:18:59.209195Z","shell.execute_reply.started":"2021-10-16T04:18:59.201800Z","shell.execute_reply":"2021-10-16T04:18:59.207801Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"# Create Logistic Regression Model","metadata":{}},{"cell_type":"code","source":"# Create Logistic Regression Model\nclass LogisticRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegressionModel, self).__init__()\n        # Linear part\n        self.linear = nn.Linear(input_dim, output_dim)\n        # There should be logistic function right?\n        # However logistic function in pytorch is in loss function\n        # So actually we do not forget to put it, it is only at next parts\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n# Instantiate Model Class\ninput_dim = 28*28 # size of image px*px\noutput_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n\n# create logistic regression model\nmodel = LogisticRegressionModel(input_dim, output_dim)\n\n# Cross Entropy Loss  \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer \nlearning_rate = 0.001\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","metadata":{"_cell_guid":"03a25584-c567-4b5e-bae1-7bc9e02184fe","_uuid":"7c7a7265a23a8101d5ed0c8826dfec3726d6161d","papermill":{"duration":0.048816,"end_time":"2021-10-15T17:36:35.780953","exception":false,"start_time":"2021-10-15T17:36:35.732137","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-16T04:18:59.211367Z","iopub.execute_input":"2021-10-16T04:18:59.211759Z","iopub.status.idle":"2021-10-16T04:18:59.225686Z","shell.execute_reply.started":"2021-10-16T04:18:59.211684Z","shell.execute_reply":"2021-10-16T04:18:59.224100Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"# Traning the Model\ncount = 0\nloss_list = []\niteration_list = []\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        # Define variables\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)   #####\n        \n        # Calculate softmax and cross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculate gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        # Prediction\n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader: \n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n                \n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct / float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {}%'.format(count, loss.data, accuracy))","metadata":{"_cell_guid":"82de08d9-7f3c-4eb9-8a99-9d7a8677799c","_uuid":"0cab9c3ec72f73db1b06578fa7a51611141e16da","papermill":{"duration":30.301357,"end_time":"2021-10-15T17:37:06.100710","exception":false,"start_time":"2021-10-15T17:36:35.799353","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-16T04:18:59.227835Z","iopub.execute_input":"2021-10-16T04:18:59.228202Z","iopub.status.idle":"2021-10-16T04:19:36.724981Z","shell.execute_reply.started":"2021-10-16T04:18:59.228158Z","shell.execute_reply":"2021-10-16T04:19:36.723998Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"# visualization\nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Logistic Regression: Loss vs Number of iteration\")\nplt.show()","metadata":{"_cell_guid":"924e9606-e155-4e39-89d3-39f941fd52f8","_uuid":"db87c03e9d263f07eb75f82a914d3e966895a6c1","papermill":{"duration":0.240508,"end_time":"2021-10-15T17:37:06.367118","exception":false,"start_time":"2021-10-15T17:37:06.126610","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-16T04:19:36.726672Z","iopub.execute_input":"2021-10-16T04:19:36.726997Z","iopub.status.idle":"2021-10-16T04:19:36.953773Z","shell.execute_reply.started":"2021-10-16T04:19:36.726955Z","shell.execute_reply":"2021-10-16T04:19:36.952719Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2021-10-16T04:19:36.954930Z","iopub.execute_input":"2021-10-16T04:19:36.955146Z","iopub.status.idle":"2021-10-16T04:19:36.968278Z","shell.execute_reply.started":"2021-10-16T04:19:36.955119Z","shell.execute_reply":"2021-10-16T04:19:36.967337Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"outputsTEST = model(featuresTEST)\npredictedTEST = torch.max(outputsTEST.data,1)[1]","metadata":{"execution":{"iopub.status.busy":"2021-10-16T04:19:36.969488Z","iopub.execute_input":"2021-10-16T04:19:36.969739Z","iopub.status.idle":"2021-10-16T04:19:37.038574Z","shell.execute_reply.started":"2021-10-16T04:19:36.969711Z","shell.execute_reply":"2021-10-16T04:19:37.037545Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"ANS=targetsTEST.numpy()\nPRED=predictedTEST.numpy()\naccuracy=accuracy_score(ANS,PRED)\nprint(accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T04:19:37.042062Z","iopub.execute_input":"2021-10-16T04:19:37.042352Z","iopub.status.idle":"2021-10-16T04:19:37.052016Z","shell.execute_reply.started":"2021-10-16T04:19:37.042319Z","shell.execute_reply":"2021-10-16T04:19:37.050992Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"markdown","source":"#### Caution!\n* linear(): argument 'input' (position 1) must be Tensor, not TensorDataset\n* TEST is TensorDataset\n* featuresTEST is Tensor","metadata":{}},{"cell_type":"markdown","source":"# Create ANN Model","metadata":{}},{"cell_type":"code","source":"# Create ANN Model\nclass ANNModel(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ANNModel, self).__init__()\n        \n        # Linear function 1: 784 --> 150\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity 1\n        self.relu1 = nn.ReLU()\n        \n        # Linear function 2: 150 --> 150\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 2\n        self.tanh2 = nn.Tanh()\n        \n        # Linear function 3: 150 --> 150\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 3\n        self.elu3 = nn.ELU()\n        \n        # Linear function 4 (readout): 150 --> 10\n        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n    \n    def forward(self, x):\n        # Linear function 1\n        out = self.fc1(x)\n        # Non-linearity 1\n        out = self.relu1(out)\n        \n        # Linear function 2\n        out = self.fc2(out)\n        # Non-linearity 2\n        out = self.tanh2(out)\n        \n        # Linear function 2\n        out = self.fc3(out)\n        # Non-linearity 2\n        out = self.elu3(out)\n        \n        # Linear function 4 (readout)\n        out = self.fc4(out)\n        return out\n\n# instantiate ANN\ninput_dim = 28*28\nhidden_dim = 150 #hidden layer dim is one of the hyper parameter and it should be chosen and tuned. For now I only say 150 there is no reason.\noutput_dim = 10\n\n# Create ANN\nmodel = ANNModel(input_dim, hidden_dim, output_dim)\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.02\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","metadata":{"_cell_guid":"3472f1c1-5888-4abe-822c-3a493a5f8be5","_uuid":"cefd0bb2f23b80f30ca65cbb08859ad81ab12e08","papermill":{"duration":0.041645,"end_time":"2021-10-15T17:37:06.544491","exception":false,"start_time":"2021-10-15T17:37:06.502846","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-16T04:19:37.053653Z","iopub.execute_input":"2021-10-16T04:19:37.053945Z","iopub.status.idle":"2021-10-16T04:19:37.069837Z","shell.execute_reply.started":"2021-10-16T04:19:37.053906Z","shell.execute_reply":"2021-10-16T04:19:37.069008Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"# ANN model training\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader:\n                \n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct / float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","metadata":{"_cell_guid":"7550e98b-5011-4d09-88ee-97b0ecbc6f19","_uuid":"c91694f3af94e4e1b76ab01489e186718c70ccd3","papermill":{"duration":56.06635,"end_time":"2021-10-15T17:38:02.635904","exception":false,"start_time":"2021-10-15T17:37:06.569554","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-16T04:19:37.071286Z","iopub.execute_input":"2021-10-16T04:19:37.071806Z","iopub.status.idle":"2021-10-16T04:20:49.199618Z","shell.execute_reply.started":"2021-10-16T04:19:37.071768Z","shell.execute_reply":"2021-10-16T04:20:49.198637Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"ANN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"ANN: Accuracy vs Number of iteration\")\nplt.show()","metadata":{"_cell_guid":"5579a7d6-7766-4d0f-b9d0-584cb4f28321","_uuid":"c5e2e6da7f1ee801e38358dc28d4c99e32d2b761","papermill":{"duration":0.441054,"end_time":"2021-10-15T17:38:03.107361","exception":false,"start_time":"2021-10-15T17:38:02.666307","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-16T04:20:49.201049Z","iopub.execute_input":"2021-10-16T04:20:49.201306Z","iopub.status.idle":"2021-10-16T04:20:49.611511Z","shell.execute_reply.started":"2021-10-16T04:20:49.201274Z","shell.execute_reply":"2021-10-16T04:20:49.610683Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2021-10-16T04:20:49.612829Z","iopub.execute_input":"2021-10-16T04:20:49.613058Z","iopub.status.idle":"2021-10-16T04:20:49.618528Z","shell.execute_reply.started":"2021-10-16T04:20:49.613028Z","shell.execute_reply":"2021-10-16T04:20:49.617684Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"outputsTEST = model(featuresTEST)\npredictedTEST = torch.max(outputsTEST.data, 1)[1]","metadata":{"execution":{"iopub.status.busy":"2021-10-16T04:20:49.619919Z","iopub.execute_input":"2021-10-16T04:20:49.620154Z","iopub.status.idle":"2021-10-16T04:20:49.670929Z","shell.execute_reply.started":"2021-10-16T04:20:49.620126Z","shell.execute_reply":"2021-10-16T04:20:49.670248Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"ANS=targetsTEST.numpy()\nPRED=predictedTEST.numpy()\naccuracy=accuracy_score(ANS,PRED)\nprint(accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T04:20:49.672138Z","iopub.execute_input":"2021-10-16T04:20:49.672935Z","iopub.status.idle":"2021-10-16T04:20:49.680347Z","shell.execute_reply.started":"2021-10-16T04:20:49.672890Z","shell.execute_reply":"2021-10-16T04:20:49.679320Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"# Create CNN Model","metadata":{}},{"cell_type":"code","source":"# Create CNN Model\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        \n        # Convolution 1\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n        self.relu1 = nn.ReLU()\n        \n        # Max pool 1\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n     \n        # Convolution 2\n        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n        self.relu2 = nn.ReLU()\n        \n        # Max pool 2\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n        \n        # Fully connected 1\n        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n    \n    def forward(self, x):\n        # Convolution 1\n        out = self.cnn1(x)\n        out = self.relu1(out)\n        \n        # Max pool 1\n        out = self.maxpool1(out)\n        \n        # Convolution 2 \n        out = self.cnn2(out)\n        out = self.relu2(out)\n        \n        # Max pool 2 \n        out = self.maxpool2(out)\n        \n        # flatten\n        out = out.view(out.size(0), -1)\n\n        # Linear function (readout)\n        out = self.fc1(out)\n        \n        return out\n\n# batch_size, epoch and iteration\nbatch_size = 100\nn_iters = 2500\nnum_epochs = n_iters / (len(features_train) / batch_size)\nnum_epochs = int(num_epochs)\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)\n    \n# Create CNN\nmodel = CNNModel()\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","metadata":{"_cell_guid":"9ca5af9e-6821-4d60-8084-edb523a39c6b","_uuid":"4915535771ffdd33ef480200393216f215b4fc48","papermill":{"duration":0.053777,"end_time":"2021-10-15T17:38:03.332412","exception":false,"start_time":"2021-10-15T17:38:03.278635","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-16T04:20:49.682325Z","iopub.execute_input":"2021-10-16T04:20:49.682694Z","iopub.status.idle":"2021-10-16T04:20:49.702168Z","shell.execute_reply.started":"2021-10-16T04:20:49.682640Z","shell.execute_reply":"2021-10-16T04:20:49.701418Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"# CNN model training\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        train = Variable(images.view(100,1,28,28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                \n                test = Variable(images.view(100,1,28,28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n                \n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct / float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","metadata":{"_cell_guid":"99a8903c-da15-496c-96b7-f5402c8fc5f0","_uuid":"f44e02d25698ac1a014795d972a384a3f3003d35","papermill":{"duration":75.401516,"end_time":"2021-10-15T17:39:18.766058","exception":false,"start_time":"2021-10-15T17:38:03.364542","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-16T04:20:49.703593Z","iopub.execute_input":"2021-10-16T04:20:49.704391Z","iopub.status.idle":"2021-10-16T04:22:25.752353Z","shell.execute_reply.started":"2021-10-16T04:20:49.704340Z","shell.execute_reply":"2021-10-16T04:22:25.751249Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"CNN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"CNN: Accuracy vs Number of iteration\")\nplt.show()","metadata":{"_cell_guid":"ac9e4aee-b8af-4641-8794-bad03b650179","_uuid":"44c1ed412d778f3e6b08f11bddc5321f63e408dd","papermill":{"duration":0.462741,"end_time":"2021-10-15T17:39:19.264402","exception":false,"start_time":"2021-10-15T17:39:18.801661","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-16T04:22:25.754114Z","iopub.execute_input":"2021-10-16T04:22:25.755042Z","iopub.status.idle":"2021-10-16T04:22:26.149173Z","shell.execute_reply.started":"2021-10-16T04:22:25.754964Z","shell.execute_reply":"2021-10-16T04:22:26.148110Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"print(featuresTEST.shape)\nprint(featuresTEST.reshape(-1,1,28,28).shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T04:22:26.150453Z","iopub.execute_input":"2021-10-16T04:22:26.150684Z","iopub.status.idle":"2021-10-16T04:22:26.156432Z","shell.execute_reply.started":"2021-10-16T04:22:26.150647Z","shell.execute_reply":"2021-10-16T04:22:26.155464Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2021-10-16T04:22:26.178136Z","iopub.execute_input":"2021-10-16T04:22:26.178417Z","iopub.status.idle":"2021-10-16T04:22:26.186103Z","shell.execute_reply.started":"2021-10-16T04:22:26.178382Z","shell.execute_reply":"2021-10-16T04:22:26.185181Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"outputsTEST = model(featuresTEST.reshape(-1,1,28,28))\npredictedTEST = torch.max(outputsTEST.data,1)[1]","metadata":{"papermill":{"duration":0.035265,"end_time":"2021-10-15T17:39:19.404648","exception":false,"start_time":"2021-10-15T17:39:19.369383","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-16T04:22:26.187559Z","iopub.execute_input":"2021-10-16T04:22:26.187916Z","iopub.status.idle":"2021-10-16T04:22:27.481759Z","shell.execute_reply.started":"2021-10-16T04:22:26.187885Z","shell.execute_reply":"2021-10-16T04:22:27.480726Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"ANS=targetsTEST.numpy()\nPRED=predictedTEST.numpy()\naccuracy=accuracy_score(ANS,PRED)\nprint(accuracy)","metadata":{"papermill":{"duration":0.034801,"end_time":"2021-10-15T17:39:19.475181","exception":false,"start_time":"2021-10-15T17:39:19.440380","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-16T04:22:27.485435Z","iopub.execute_input":"2021-10-16T04:22:27.485713Z","iopub.status.idle":"2021-10-16T04:22:27.493276Z","shell.execute_reply.started":"2021-10-16T04:22:27.485681Z","shell.execute_reply":"2021-10-16T04:22:27.492168Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.036452,"end_time":"2021-10-15T17:39:19.546459","exception":false,"start_time":"2021-10-15T17:39:19.510007","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}